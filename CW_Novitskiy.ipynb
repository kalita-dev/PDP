{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3db85660",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T06:17:10.779909Z",
     "iopub.status.busy": "2024-04-26T06:17:10.779479Z",
     "iopub.status.idle": "2024-04-26T06:17:10.809806Z",
     "shell.execute_reply": "2024-04-26T06:17:10.809038Z",
     "shell.execute_reply.started": "2024-04-26T06:17:10.779875Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install opendatasets --upgrade --quiet\n",
    "# import opendatasets as od\n",
    "# dataset = 'https://www.kaggle.com/datasets/wcukierski/enron-email-dataset'\n",
    "# od.download(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13846b9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T06:17:10.828524Z",
     "iopub.status.busy": "2024-04-26T06:17:10.827997Z",
     "iopub.status.idle": "2024-04-26T06:17:10.840149Z",
     "shell.execute_reply": "2024-04-26T06:17:10.839189Z",
     "shell.execute_reply.started": "2024-04-26T06:17:10.828494Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text_from_email(msg):\n",
    "    '''To get the content from email objects'''\n",
    "    parts = []\n",
    "    for part in msg.walk():\n",
    "        if part.get_content_type() == 'text/plain':\n",
    "            parts.append( part.get_payload() )\n",
    "    return ''.join(parts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ad4a6f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T06:17:10.841476Z",
     "iopub.status.busy": "2024-04-26T06:17:10.841194Z",
     "iopub.status.idle": "2024-04-26T06:17:37.425621Z",
     "shell.execute_reply": "2024-04-26T06:17:37.424642Z",
     "shell.execute_reply.started": "2024-04-26T06:17:10.841454Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/enron-email-dataset/emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf673f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T06:17:37.433960Z",
     "iopub.status.busy": "2024-04-26T06:17:37.433518Z",
     "iopub.status.idle": "2024-04-26T06:17:39.198311Z",
     "shell.execute_reply": "2024-04-26T06:17:39.197372Z",
     "shell.execute_reply.started": "2024-04-26T06:17:37.433927Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, email\n",
    "df['message'] = list(map(get_text_from_email, list(map(email.message_from_string, df['message']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d9ca0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T06:17:39.199814Z",
     "iopub.status.busy": "2024-04-26T06:17:39.199538Z",
     "iopub.status.idle": "2024-04-26T06:17:48.511261Z",
     "shell.execute_reply": "2024-04-26T06:17:48.510194Z",
     "shell.execute_reply.started": "2024-04-26T06:17:39.199790Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-26 06:17:41.425077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-26 06:17:41.425206: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-26 06:17:41.557067: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, MultiHeadAttention, Dropout, LayerNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "class TransformerBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(ff_dim, activation=\"relu\"), \n",
    "            Dense(embed_dim),\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "def create_model(vocab_size, embed_dim, num_heads, ff_dim, maxlen):\n",
    "    inputs = Input(shape=(maxlen,))\n",
    "    embedding_layer = Embedding(vocab_size, embed_dim)(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "    x = transformer_block(embedding_layer)\n",
    "    outputs = Dense(vocab_size)(x)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b028ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-26T06:17:48.514938Z",
     "iopub.status.busy": "2024-04-26T06:17:48.514385Z",
     "iopub.status.idle": "2024-04-26T06:26:14.402095Z",
     "shell.execute_reply": "2024-04-26T06:26:14.401244Z",
     "shell.execute_reply.started": "2024-04-26T06:17:48.514912Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1714112277.460848     198 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 69s 6ms/step - loss: 1.6525\n",
      "Epoch 2/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.9158\n",
      "Epoch 3/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.8540\n",
      "Epoch 4/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.8295\n",
      "Epoch 5/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.8161\n",
      "Epoch 6/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.8035\n",
      "Epoch 7/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.7904\n",
      "Epoch 8/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.7807\n",
      "Epoch 9/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.7743\n",
      "Epoch 10/10\n",
      "10000/10000 [==============================] - 48s 5ms/step - loss: 0.7662\n"
     ]
    }
   ],
   "source": [
    "texts = df.message.values\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "seqs = tokenizer.texts_to_sequences(texts)\n",
    "maxlen = max([len(i.split(' ')) for i in texts])\n",
    "seqs = tf.keras.preprocessing.sequence.pad_sequences(seqs, maxlen=maxlen, padding='post')\n",
    "\n",
    "def mask_input(seqs, mask_prob=0.15):\n",
    "    random_masks = np.random.rand(*seqs.shape) < mask_prob\n",
    "    masked_seqs = np.where(random_masks, 32365, seqs) \n",
    "    return masked_seqs, seqs\n",
    "\n",
    "masked_inputs, labels = mask_input(seqs)\n",
    "with tf.device('/gpu:0'):\n",
    "    model = create_model(vocab_size, embed_dim=32, num_heads=2, ff_dim=32, maxlen=maxlen)\n",
    "    model.compile(optimizer=\"adam\", loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
    "    model.fit(masked_inputs, labels, epochs=10, batch_size=2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 55,
     "sourceId": 120,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
